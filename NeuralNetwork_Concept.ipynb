{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    '''A single neuron with the sigmoid activation function with the below attributes:\n",
    "        inputs: Total no. of inputs into the perceptron\n",
    "        bias: The bias term which is by default kept as 1.0''' \n",
    "\n",
    "    def __init__(self, inputs, bias=1.0):\n",
    "        # Returns a perceptron object with the total no. of inputs and the bias\n",
    "        self.weights = (np.random.rand(inputs+1) * 2) - 1\n",
    "        self.bias = bias\n",
    "        \n",
    "        \n",
    "    def run(self, x):\n",
    "        # Run the perceptron with the list of inputs(x)\n",
    "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
    "        # Use sigmoid activation function\n",
    "        return self.sigmoid(sum)\n",
    "    \n",
    "    def set_weights(self, w_init):\n",
    "        # w_init : list of floats\n",
    "        self.weights = np.array(w_init)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Return the output of the activation function\n",
    "        return(1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Gate\n",
      "For input 0 and 0, output = 0.00\n",
      "For input 0 and 1, output = 0.00\n",
      "For input 1 and 0, output = 0.00\n",
      "For input 1 and 1, output = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test the perceptron\n",
    "# AND Gate\n",
    "\n",
    "percep = Perceptron(inputs=2)\n",
    "percep.set_weights([2.3,3.2,-5.0])\n",
    "\n",
    "print('Logic Gate AND')\n",
    "print('For input 0 and 0, output = {0:.2f}'.format(round(percep.run([0,0]),0)))\n",
    "print('For input 0 and 1, output = {0:.2f}'.format(round(percep.run([0,1]),0)))\n",
    "print('For input 1 and 0, output = {0:.2f}'.format(round(percep.run([1,0]),0)))\n",
    "print('For input 1 and 1, output = {0:.2f}'.format(round(percep.run([1,1]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Gate OR\n",
      "For input 0 and 0, output = 0.00\n",
      "For input 0 and 1, output = 1.00\n",
      "For input 1 and 0, output = 1.00\n",
      "For input 1 and 1, output = 1.00\n"
     ]
    }
   ],
   "source": [
    "# OR Gate\n",
    "\n",
    "percep.set_weights([1.2,3.2,-0.5])\n",
    "\n",
    "print('Logic Gate OR')\n",
    "print('For input 0 and 0, output = {0:.2f}'.format(round(percep.run([0,0]),0)))\n",
    "print('For input 0 and 1, output = {0:.2f}'.format(round(percep.run([0,1]),0)))\n",
    "print('For input 1 and 0, output = {0:.2f}'.format(round(percep.run([1,0]),0)))\n",
    "print('For input 1 and 1, output = {0:.2f}'.format(round(percep.run([1,1]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Gate NAND\n",
      "For input 0 and 0, output = 1.00\n",
      "For input 0 and 1, output = 1.00\n",
      "For input 1 and 0, output = 1.00\n",
      "For input 1 and 1, output = 0.00\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate\n",
    "\n",
    "percep = Perceptron(inputs=2)\n",
    "percep.set_weights([-4.3,-4.2,5.5])\n",
    "\n",
    "print('Logic Gate NAND')\n",
    "print('For input 0 and 0, output = {0:.2f}'.format(round(percep.run([0,0]),0)))\n",
    "print('For input 0 and 1, output = {0:.2f}'.format(round(percep.run([0,1]),0)))\n",
    "print('For input 1 and 0, output = {0:.2f}'.format(round(percep.run([1,0]),0)))\n",
    "print('For input 1 and 1, output = {0:.2f}'.format(round(percep.run([1,1]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOR Gate\n",
    "\n",
    "percep = Perceptron(inputs=2)\n",
    "percep.set_weights([-2.3,-3.2,0.2])\n",
    "\n",
    "print('Logic Gate NOR')\n",
    "print('For input 0 and 0, output = {0:.2f}'.format(round(percep.run([0,0]),0)))\n",
    "print('For input 0 and 1, output = {0:.2f}'.format(round(percep.run([0,1]),0)))\n",
    "print('For input 1 and 0, output = {0:.2f}'.format(round(percep.run([1,0]),0)))\n",
    "print('For input 1 and 1, output = {0:.2f}'.format(round(percep.run([1,1]),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:     \n",
    "    \"\"\"A multilayer perceptron class that uses the Perceptron class above.\n",
    "       Attributes:\n",
    "          layers:  A python list with the number of elements per layer.\n",
    "          bias:    The bias term. The same bias is used for all neurons.\n",
    "          eta:     The learning rate.\"\"\"\n",
    "\n",
    "    def __init__(self, layers, bias = 1.0, eta = 0.5):\n",
    "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
    "        self.layers = np.array(layers,dtype=object)\n",
    "        self.bias = bias\n",
    "        self.eta = eta\n",
    "        self.network = [] # The list of lists of neurons\n",
    "        self.values = []  # The list of lists of output values\n",
    "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            self.values.append([])\n",
    "            self.d.append([])\n",
    "            self.network.append([])\n",
    "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
    "            self.d[i] = [0.0 for j in range(self.layers[i])]\n",
    "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
    "                for j in range(self.layers[i]): \n",
    "                    self.network[i].append(Perceptron(inputs = self.layers[i-1], bias = self.bias))\n",
    "        \n",
    "        self.network = np.array([np.array(x) for x in self.network],dtype=object)\n",
    "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
    "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
    "\n",
    "    def set_weights(self, w_init):\n",
    "        \"\"\"Set the weights. \n",
    "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
    "        for i in range(len(w_init)):\n",
    "            for j in range(len(w_init[i])):\n",
    "                self.network[i+1][j].set_weights(w_init[i][j])\n",
    "\n",
    "    def printWeights(self):\n",
    "        print()\n",
    "        for i in range(1,len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                print(\"Layer\",i+1,\"Neuron\",j,self.network[i][j].weights)\n",
    "        print()\n",
    "\n",
    "    def run(self, x):\n",
    "        \"\"\"Feed a sample x into the MultiLayer Perceptron.\"\"\"\n",
    "        x = np.array(x,dtype=object)\n",
    "        self.values[0] = x\n",
    "        for i in range(1,len(self.network)):\n",
    "            for j in range(self.layers[i]):  \n",
    "                self.values[i][j] = self.network[i][j].run(self.values[i-1])\n",
    "        return self.values[-1]\n",
    "    \n",
    "    def bp(self, x, y):\n",
    "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm.\"\"\"\n",
    "        x = np.array(x,dtype=object)\n",
    "        y = np.array(y,dtype=object)\n",
    "\n",
    "        # Backpropagation Step by Step:\n",
    "\n",
    "        # STEP 1: Feed a sample to the network \n",
    "        outputs = self.run(x)\n",
    "        \n",
    "        # STEP 2: Calculate the MSE\n",
    "        error = (y - outputs)\n",
    "        MSE = sum( error ** 2) / self.layers[-1]\n",
    "\n",
    "        # STEP 3: Calculate the output error terms\n",
    "        self.d[-1] = outputs * (1 - outputs) * (error)\n",
    "\n",
    "        # STEP 4: Calculate the error term of each unit on each layer\n",
    "        for i in reversed(range(1,len(self.network)-1)):\n",
    "            for h in range(len(self.network[i])):\n",
    "                fwd_error = 0.0\n",
    "                for k in range(self.layers[i+1]): \n",
    "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
    "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
    "\n",
    "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
    "        for i in range(1,len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                for k in range(self.layers[i-1]+1):\n",
    "                    if k==self.layers[i-1]:\n",
    "                        delta = self.eta * self.d[i][j] * self.bias\n",
    "                    else:\n",
    "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
    "                    self.network[i][j].weights[k] += delta\n",
    "        return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
